{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA with num_topics=6 and alpha=symmetric...\n",
      "Coherence Score: 0.09198450362067177\n",
      "Training LDA with num_topics=6 and alpha=asymmetric...\n",
      "Coherence Score: 0.09468709841055484\n",
      "Training LDA with num_topics=6 and alpha=auto...\n",
      "Coherence Score: 0.09508168361344975\n",
      "Training LDA with num_topics=8 and alpha=symmetric...\n",
      "Coherence Score: 0.1006117632666429\n",
      "Training LDA with num_topics=8 and alpha=asymmetric...\n",
      "Coherence Score: 0.1134339642074523\n",
      "Training LDA with num_topics=8 and alpha=auto...\n",
      "Coherence Score: 0.09966981054115416\n",
      "Training LDA with num_topics=9 and alpha=symmetric...\n",
      "Coherence Score: 0.09228134002104992\n",
      "Training LDA with num_topics=9 and alpha=asymmetric...\n",
      "Coherence Score: 0.09140630395775151\n",
      "Training LDA with num_topics=9 and alpha=auto...\n",
      "Coherence Score: 0.09997840204222881\n",
      "Training LDA with num_topics=10 and alpha=symmetric...\n",
      "Coherence Score: 0.09168175684418124\n",
      "Training LDA with num_topics=10 and alpha=asymmetric...\n",
      "Coherence Score: 0.08112771732938608\n",
      "Training LDA with num_topics=10 and alpha=auto...\n",
      "Coherence Score: 0.11364368102180053\n",
      "Best Coherence Score: 0.11364368102180053 with parameters {'num_topics': 10, 'alpha': 'auto'}\n",
      "Best model coherence score: 0.11364368102180053\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 144\u001b[0m\n\u001b[0;32m    141\u001b[0m     merged_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 137\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    134\u001b[0m save_model_and_scores(best_lda_model, dictionary, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlda_models\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdictionaries\u001b[39m\u001b[38;5;124m\"\u001b[39m, coherence_df, prompt_id)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Get topic distributions and save highest topic\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m final_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_topic_distributions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_lda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# Combine with handcrafted features and save final merged dataset\u001b[39;00m\n\u001b[0;32m    140\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m combine_with_handcrafted(final_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhand_crafted_v3.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 83\u001b[0m, in \u001b[0;36mget_topic_distributions\u001b[1;34m(df, lda_model, dictionary, prompt_id)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_topic_distributions\u001b[39m(df, lda_model, dictionary, prompt_id):\n\u001b[1;32m---> 83\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_texts\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemmatizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     topic_distributions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     85\u001b[0m     highest_topic_scores \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\mishr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mishr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mishr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\mishr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mishr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[2], line 83\u001b[0m, in \u001b[0;36mget_topic_distributions.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_topic_distributions\u001b[39m(df, lda_model, dictionary, prompt_id):\n\u001b[1;32m---> 83\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_texts\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: preprocess_text(x, \u001b[43mstop_words\u001b[49m, lemmatizer))\n\u001b[0;32m     84\u001b[0m     topic_distributions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     85\u001b[0m     highest_topic_scores \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop_words' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "\n",
    "# Load and preprocess the data\n",
    "def load_and_preprocess(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['content_text'] = df['content_text'].str.lower()\n",
    "    df['content_text'] = df['content_text'].str.replace(r'@[\\w]+', '', regex=True)\n",
    "    df['content_text'] = df['content_text'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "    df['content_text'] = df['content_text'].str.replace(r'\\s+', ' ', regex=True)\n",
    "    df['content_text'] = df['content_text'].str.strip()\n",
    "    df['content_text'] = df['content_text'].str.replace(r'[\"\\']', '', regex=True)\n",
    "    return df\n",
    "\n",
    "# Tokenize, remove stopwords, and lemmatize\n",
    "def preprocess_text(text, stop_words, lemmatizer):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Prepare text for LDA (split into train/test, create dictionary and corpus)\n",
    "def prepare_lda_data(df, prompt_id, stop_words, lemmatizer):\n",
    "    df['processed_texts'] = df['content_text'].apply(lambda x: preprocess_text(x, stop_words, lemmatizer))\n",
    "    train_df = df[df['prompt_id'] != prompt_id]\n",
    "    \n",
    "    dictionary = corpora.Dictionary(train_df['processed_texts'])\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "    corpus = [dictionary.doc2bow(text) for text in train_df['processed_texts']]\n",
    "    return train_df, dictionary, corpus\n",
    "\n",
    "# Train the LDA model\n",
    "def train_lda(corpus, dictionary, num_topics, alpha='auto', passes=100, iterations=400):\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, \n",
    "                         random_state=42, alpha=alpha, passes=passes, iterations=iterations)\n",
    "    return lda_model\n",
    "\n",
    "# Evaluate topics using coherence scores\n",
    "def evaluate_topics(lda_model, texts, dictionary, coherence_metric='c_npmi'):\n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence=coherence_metric)\n",
    "    overall_coherence = coherence_model.get_coherence()\n",
    "    return overall_coherence\n",
    "\n",
    "# Grid search for best LDA model parameters\n",
    "def grid_search_lda(corpus, dictionary, texts, num_topics_grid, alpha_grid, coherence_metric='c_npmi'):\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "    best_params = {}\n",
    "    \n",
    "    for num_topics in num_topics_grid:\n",
    "        for alpha in alpha_grid:\n",
    "            print(f\"Training LDA with num_topics={num_topics} and alpha={alpha}...\")\n",
    "            lda_model = train_lda(corpus, dictionary, num_topics=num_topics, alpha=alpha)\n",
    "            coherence_score = evaluate_topics(lda_model, texts, dictionary, coherence_metric)\n",
    "            print(f\"Coherence Score: {coherence_score}\")\n",
    "            \n",
    "            # Check if current model is better than the previous best\n",
    "            if coherence_score > best_score:\n",
    "                best_score = coherence_score\n",
    "                best_model = lda_model\n",
    "                best_params = {'num_topics': num_topics, 'alpha': alpha}\n",
    "    \n",
    "    print(f\"Best Coherence Score: {best_score} with parameters {best_params}\")\n",
    "    return best_model, best_params, best_score\n",
    "\n",
    "# Save LDA model, dictionary, and coherence scores\n",
    "def save_model_and_scores(lda_model, dictionary, model_dir, dict_dir, coherence_df, prompt_id):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(dict_dir, exist_ok=True)\n",
    "    lda_model.save(os.path.join(model_dir, f\"lda_model_{prompt_id}.model\"))\n",
    "    dictionary.save(os.path.join(dict_dir, f\"dictionary_{prompt_id}.dict\"))\n",
    "    coherence_df.to_csv(f'topic_coherence_scores_{prompt_id}.csv', index=False)\n",
    "\n",
    "# Get topic distributions for test data and save highest topic\n",
    "def get_topic_distributions(df, lda_model, dictionary, prompt_id):\n",
    "    df['processed_texts'] = df['content_text'].apply(lambda x: preprocess_text(x, stop_words, lemmatizer))\n",
    "    topic_distributions = []\n",
    "    highest_topic_scores = []\n",
    "\n",
    "    for text in df['processed_texts']:\n",
    "        new_bow = dictionary.doc2bow(text)\n",
    "        topic_distribution = lda_model.get_document_topics(new_bow)\n",
    "        topic_distributions.append(topic_distribution)\n",
    "        highest_topic_scores.append(max(topic_distribution, key=lambda x: x[1])[1])\n",
    "\n",
    "    df['highest_topic'] = highest_topic_scores\n",
    "    return df\n",
    "\n",
    "# Combine with handcrafted features\n",
    "def combine_with_handcrafted(final_df, handcrafted_file):\n",
    "    handcrafted_df = pd.read_csv(handcrafted_file)\n",
    "    final_df.rename(columns={'essay_id': 'item_id'}, inplace=True)\n",
    "    merged_df = handcrafted_df.merge(final_df[['item_id', 'highest_topic']], on='item_id')\n",
    "    return merged_df\n",
    "\n",
    "# Main function to run each step\n",
    "def main():\n",
    "    # Load and preprocess\n",
    "    file_path = 'combined_data.csv'\n",
    "    df = load_and_preprocess(file_path)\n",
    "\n",
    "    # Set parameters and stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    prompt_id = 1\n",
    "    \n",
    "    # Grid search parameters\n",
    "    num_topics_grid = [6, 8, 9, 10]  # Define your topic numbers for grid search\n",
    "    alpha_grid = ['symmetric', 'asymmetric', 'auto']  # Define your alpha values\n",
    "    \n",
    "    # Prepare LDA data\n",
    "    train_df, dictionary, corpus = prepare_lda_data(df, prompt_id, stop_words, lemmatizer)\n",
    "\n",
    "    # Grid search for best model\n",
    "    best_lda_model, best_params, best_score = grid_search_lda(corpus, dictionary, train_df['processed_texts'], num_topics_grid, alpha_grid)\n",
    "    \n",
    "    # Evaluate and print final coherence score with best parameters\n",
    "    final_coherence = evaluate_topics(best_lda_model, train_df['processed_texts'], dictionary)\n",
    "    print(f\"Best model coherence score: {final_coherence}\")\n",
    "\n",
    "    # Save best model and coherence scores\n",
    "    coherence_df = pd.DataFrame({\n",
    "        'Topic': range(best_params['num_topics']),\n",
    "        'Coherence Score': evaluate_topics(best_lda_model, train_df['processed_texts'], dictionary, coherence_metric='c_npmi'),\n",
    "        'word_dist': best_lda_model.print_topics(num_words=10)\n",
    "    })\n",
    "    save_model_and_scores(best_lda_model, dictionary, \"lda_models\", \"dictionaries\", coherence_df, prompt_id)\n",
    "\n",
    "    # Get topic distributions and save highest topic\n",
    "    final_df = get_topic_distributions(df, best_lda_model, dictionary, prompt_id)\n",
    "\n",
    "    # Combine with handcrafted features and save final merged dataset\n",
    "    merged_df = combine_with_handcrafted(final_df, 'hand_crafted_v3.csv')\n",
    "    merged_df.to_csv(f'final_{prompt_id}.csv', index=False)\n",
    "\n",
    "# Run the main function\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Load and preprocess the data\n",
    "def load_and_preprocess(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['content_text'] = df['content_text'].str.lower()\n",
    "    df['content_text'] = df['content_text'].str.replace(r'@[\\w]+', '', regex=True)\n",
    "    df['content_text'] = df['content_text'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "    df['content_text'] = df['content_text'].str.replace(r'\\s+', ' ', regex=True)\n",
    "    df['content_text'] = df['content_text'].str.strip()\n",
    "    df['content_text'] = df['content_text'].str.replace(r'[\"\\']', '', regex=True)\n",
    "    return df\n",
    "\n",
    "# Tokenize, remove stopwords, and lemmatize\n",
    "def preprocess_text(text, stop_words, lemmatizer):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Prepare BERTopic model and fit it\n",
    "def train_bertopic(df, prompt_id, stop_words, lemmatizer):\n",
    "    df['processed_texts'] = df['content_text'].apply(lambda x: preprocess_text(x, stop_words, lemmatizer))\n",
    "    train_df = df[df['prompt_id'] != prompt_id]\n",
    "\n",
    "    # Initialize BERTopic with custom vectorizer (optional, for better control)\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "    topic_model = BERTopic(vectorizer_model=vectorizer_model)\n",
    "\n",
    "    topics, probabilities = topic_model.fit_transform(train_df['processed_texts'])\n",
    "    return topic_model, topics, probabilities\n",
    "\n",
    "# Evaluate topics using silhouette score\n",
    "def evaluate_topics_bertopic(probabilities):\n",
    "    silhouette_avg = silhouette_score(probabilities, probabilities.argmax(axis=1))\n",
    "    return silhouette_avg\n",
    "\n",
    "# Save BERTopic model\n",
    "def save_bertopic_model(topic_model, model_dir, prompt_id):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    topic_model.save(os.path.join(model_dir, f\"bertopic_model_{prompt_id}\"))\n",
    "\n",
    "# Get topic distributions for test data and save highest topic\n",
    "def get_topic_distributions(df, topic_model, prompt_id):\n",
    "    df['processed_texts'] = df['content_text'].apply(lambda x: preprocess_text(x, stop_words, lemmatizer))\n",
    "    topics, probabilities = topic_model.transform(df['processed_texts'])\n",
    "    \n",
    "    highest_topic_scores = probabilities.max(axis=1)\n",
    "    df['highest_topic'] = highest_topic_scores\n",
    "    return df\n",
    "\n",
    "# Combine with handcrafted features\n",
    "def combine_with_handcrafted(final_df, handcrafted_file):\n",
    "    handcrafted_df = pd.read_csv(handcrafted_file)\n",
    "    final_df.rename(columns={'essay_id': 'item_id'}, inplace=True)\n",
    "    merged_df = handcrafted_df.merge(final_df[['item_id', 'highest_topic']], on='item_id')\n",
    "    return merged_df\n",
    "\n",
    "# Main function to run each step\n",
    "def main():\n",
    "    # Load and preprocess\n",
    "    file_path = 'combined_data.csv'\n",
    "    df = load_and_preprocess(file_path)\n",
    "\n",
    "    # Set parameters and stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    prompt_id = 1\n",
    "\n",
    "    # Train BERTopic model\n",
    "    topic_model, topics, probabilities = train_bertopic(df, prompt_id, stop_words, lemmatizer)\n",
    "    \n",
    "    # Evaluate topic coherence\n",
    "    silhouette_avg = evaluate_topics_bertopic(probabilities)\n",
    "    print(f\"Silhouette Score for BERTopic model: {silhouette_avg}\")\n",
    "\n",
    "    # Save BERTopic model\n",
    "    save_bertopic_model(topic_model, \"bertopic_models\", prompt_id)\n",
    "\n",
    "    # Get topic distributions and save highest topic\n",
    "    final_df = get_topic_distributions(df, topic_model, prompt_id)\n",
    "\n",
    "    # Combine with handcrafted features and save final merged dataset\n",
    "    merged_df = combine_with_handcrafted(final_df, 'hand_crafted_v3.csv')\n",
    "    merged_df.to_csv(f'final_{prompt_id}.csv', index=False)\n",
    "\n",
    "# Run the main function\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
